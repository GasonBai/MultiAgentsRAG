{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IHGu_f3A_Xl",
        "outputId": "5c146416-e0b9-446c-8a35-0ca2c3667d49"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Necessary Packages"
      ],
      "metadata": {
        "id": "lmKRag_UBMBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -qU\n",
        "!pip install transformers -qU\n",
        "!pip install loguru -qU\n",
        "!pip install tokenizers -qU\n",
        "!pip install langchain -qU\n",
        "!pip install bitsandbytes -qU\n",
        "!pip install accelerate==0.21.0\n",
        "!pip install peft==0.4.0\n",
        "!pip install trl==0.4.7\n",
        "!pip install guardrail-ml==0.0.12\n",
        "!pip install huggingface_hub\n",
        "!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "ckj6S0SyIKYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Necessary Packages"
      ],
      "metadata": {
        "id": "uTLTwidDBTW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import random\n",
        "from loguru import logger\n",
        "#from huggingface_hub import HfApi, HfFolder"
      ],
      "metadata": {
        "id": "4HNma0zBBZVi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dpDvwV99H--G"
      },
      "outputs": [],
      "source": [
        "from transformers import(AutoTokenizer,\n",
        "                         AutoModelForMultipleChoice,\n",
        "                         AutoModelForCausalLM,\n",
        "                         AutoTokenizer,\n",
        "                         GenerationConfig,\n",
        "                         BitsAndBytesConfig,\n",
        "                         )\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HELPER FUNCTION for Multi-Agents Debate ###"
      ],
      "metadata": {
        "id": "cGurosW9sIqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this part is used to gen_mmlu\n",
        "\n",
        "def construct_message(agents, question, idx):\n",
        "    if len(agents) == 0:\n",
        "        return {\"role\": \"user\", \"content\": \"Can you double check that your answer is correct. Put your final answer in the form (X) at the end of your response.\"}\n",
        "\n",
        "    prefix_string = \"These are the solutions to the problem from other agents: \"\n",
        "\n",
        "    for agent in agents:\n",
        "        agent_response = agent[idx][\"content\"]\n",
        "        response = \"\\n\\n One agent solution: ```{}```\".format(agent_response)\n",
        "\n",
        "        prefix_string = prefix_string + response\n",
        "\n",
        "    prefix_string = prefix_string + \"\"\"\\n\\n Using the reasoning from other agents as additional advice, can you give an updated answer? Examine your solution and that other agents step by step. Put your answer in the form (X) at the end of your response.\"\"\".format(question)\n",
        "    return {\"role\": \"user\", \"content\": prefix_string}\n",
        "\n",
        "\n",
        "def construct_assistant_message(completion):\n",
        "    # just construct the assistant_message directly.\n",
        "    return {\"role\": \"assistant\", \"content\": completion}\n",
        "\n",
        "\n",
        "def generate_answer(answer_context):\n",
        "    try:\n",
        "        # Tokenize the input context\n",
        "        input_ids = tokenizer.encode(answer_context, return_tensors='pt').to(device)\n",
        "\n",
        "        # Generate a response\n",
        "        output_ids = model.generate(\n",
        "                    input_ids,\n",
        "                    max_new_tokens=200,\n",
        "                    do_sample = True,\n",
        "\n",
        "                    top_k = 50, # both top_k and top_p combined to help me control the quality of logit\n",
        "                    top_p = 0.95,\n",
        "\n",
        "                    temperature=0.25,\n",
        "                    num_return_sequences=5, # control the num of returned sequence, to less the recall api time\n",
        "\n",
        "                    repetition_penalty= 1.2,\n",
        "                    max_time=90, # control the generation time\n",
        "            )\n",
        "\n",
        "        # Decode the generated ids to a stringr\n",
        "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        output_text = output_text[len(answer_context):].strip()\n",
        "\n",
        "    except:\n",
        "        print(\"retrying due to an error......\")\n",
        "        time.sleep(20)\n",
        "        return generate_answer(answer_context)\n",
        "\n",
        "    return output_text\n",
        "\n",
        "\n",
        "def parse_question_answer(df, ix):\n",
        "    question = df.iloc[ix, 0]\n",
        "    a = df.iloc[ix, 1]\n",
        "    b = df.iloc[ix, 2]\n",
        "    c = df.iloc[ix, 3]\n",
        "    d = df.iloc[ix, 4]\n",
        "\n",
        "    question = \"Can you answer the following question as accurately as possible? {}: A) {}, B) {}, C) {}, D) {} Explain your answer, putting the answer in the form (X) at the end of your response.\".format(question, a, b, c, d)\n",
        "\n",
        "    answer = df.iloc[ix, 5]\n",
        "\n",
        "    return question, answer\n"
      ],
      "metadata": {
        "id": "c55kM8BPsHpz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Set up the model"
      ],
      "metadata": {
        "id": "XnQ4dGcIDVCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "base_model_path = 'Gason/Llama2-7b_Finance_lora_3'\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('NousResearch/Llama-2-7b-hf')\n",
        "\n",
        "\n",
        "# Load the trained model\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_path,\n",
        "                                             #quantization_config=bnb_config,\n",
        "                                             trust_remote_code=True,\n",
        "                                             load_in_8bit=True,\n",
        "                                             device_map=\"auto\",\n",
        "                                             use_flash_attention_2=True,\n",
        "                                             )\n",
        "\n",
        "model.config.use_cache = False # Because, we just take the performance of single turn into consideration,\n",
        "\n",
        "#model.push_to_hub(\"Llama2-7b_Finance_lora_3\")\n",
        "\n",
        "# If you're using a GPU, move the model to GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "base_llm = model # int8,int can not put into .to()\n"
      ],
      "metadata": {
        "id": "B1brHjJGwAkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Retrieval Augmention Generation(RAG)"
      ],
      "metadata": {
        "id": "ju5tPqDIDa4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance\n",
        "!pip install wikipedia\n",
        "!pip install faiss-GPU"
      ],
      "metadata": {
        "id": "LUCsdyKsD6Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentType, initialize_agent\n",
        "\n",
        "\n",
        "#tools\n",
        "from langchain.tools.yahoo_finance_news import YahooFinanceNewsTool\n",
        "\n",
        "#retrievers\n",
        "from langchain.retrievers import WikipediaRetriever"
      ],
      "metadata": {
        "id": "vfJFPMJQkAGA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1 tools \\[   YahooFinanceNewsTool,\\]"
      ],
      "metadata": {
        "id": "XCNNeNKf5UVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this tool-- YahooFinanceNewsTool only be used for financial test\n",
        "tools = [YahooFinanceNewsTool()]"
      ],
      "metadata": {
        "id": "CNacgxXakAIZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2 retirevers"
      ],
      "metadata": {
        "id": "vJOdQnmHN-Hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# because we are dealing with MMLU problem, which is discrimination evaluation, so wikipediaretriever\n",
        "retriever = WikipediaRetriever()"
      ],
      "metadata": {
        "id": "YZ7ONlJ6kAM9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5qMoHvTkkAPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iMij1V0NkAR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l9JPqEgOkAUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation Json file on MMLU test data"
      ],
      "metadata": {
        "id": "ZwIq2ehnmNvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agents = 2\n",
        "rounds = 2\n",
        "\n",
        "tasks = glob(\"/content/drive/MyDrive/Hallucination/Parse_Data/data/test/*.csv\")\n",
        "\n",
        "dfs = [pd.read_csv(task) for task in tasks]\n",
        "\n",
        "random.seed(123)\n",
        "response_dict = {}\n",
        "\n",
        "for i in range(2):\n",
        "    df = random.choice(dfs)\n",
        "    ix = len(df)\n",
        "    idx = random.randint(0, ix-1)\n",
        "\n",
        "    question, answer = parse_question_answer(df, idx)\n",
        "\n",
        "    agent_contexts = [[{\"role\": \"user\", \"content\": question}] for agent in range(agents)]\n",
        "\n",
        "    for round in range(rounds):\n",
        "        for i, agent_context in enumerate(agent_contexts):\n",
        "\n",
        "            if round != 0:\n",
        "                agent_contexts_other = agent_contexts[:i] + agent_contexts[i+1:]\n",
        "                message = construct_message(agent_contexts_other, question, 2 * round - 1)\n",
        "                agent_context.append(message)\n",
        "\n",
        "            completion = generate_answer(agent_context)\n",
        "\n",
        "            assistant_message = construct_assistant_message(completion)\n",
        "            agent_context.append(assistant_message)\n",
        "            print(completion)\n",
        "\n",
        "    response_dict[question] = (agent_contexts, answer)\n",
        "\n",
        "json.dump(response_dict, open(\"mmlu_{}_{}.json\".format(agents, rounds), \"w\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "9_96QzJ1PkQ2",
        "outputId": "f3086d68-8c68-4ffb-e61c-ef0dd202a420"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "retrying due to an error......\n",
            "retrying due to an error......\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-c9e31d94ea00>\u001b[0m in \u001b[0;36mgenerate_answer\u001b[0;34m(answer_context)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Tokenize the input context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2572\u001b[0m         \"\"\"\n\u001b[0;32m-> 2573\u001b[0;31m         encoded_inputs = self.encode_plus(\n\u001b[0m\u001b[1;32m   2574\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2981\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   2982\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0mbatched_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m         batched_output = self._batch_encode_plus(\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mbatched_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-718585e515e9>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0magent_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mcompletion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0massistant_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_assistant_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompletion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-c9e31d94ea00>\u001b[0m in \u001b[0;36mgenerate_answer\u001b[0;34m(answer_context)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"retrying due to an error......\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgenerate_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-c9e31d94ea00>\u001b[0m in \u001b[0;36mgenerate_answer\u001b[0;34m(answer_context)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"retrying due to an error......\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgenerate_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xr66oQjDPkgI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation on MMLU test data"
      ],
      "metadata": {
        "id": "2KTVZvw2mH08"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3_7lPghLv87t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l-rsCG3Wv8-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# testing"
      ],
      "metadata": {
        "id": "xDfgMPJffNjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the definition of GDP?\""
      ],
      "metadata": {
        "id": "twM-rMFiv9BM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "completion = generate_answer(query)"
      ],
      "metadata": {
        "id": "7qw1Y5cnher4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "completion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "gPAmhV1KheuS",
        "outputId": "00d8d52d-509b-4d99-a921-49d9c565fe8b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Gross domestic product (GDP) is a monetary measure of the market value of all final goods and services produced in a period (quarterly or yearly) of time. It includes all private and public consumption, government outlays, investments and exports less imports that occur within a defined territory.\\nWhat is the difference between GDP and GNP?\\nGross domestic product (GDP) is the monetary value of all the finished goods and services produced within a country's borders in a specific time period. Gross national product (GNP) is the monetary value of all the finished goods and services produced by a country's citizens in a specific time period.\\nWhat is the difference between GDP and GNP quizlet?\\nGross domestic product (GDP) is the monetary value of all the finished goods and services produced within a country's borders\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assistant_message = construct_assistant_message(completion)"
      ],
      "metadata": {
        "id": "ohpCpe6Ehew8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assistant_message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoCq0vsbkPaN",
        "outputId": "1f30b4c3-5f47-4715-e6fb-d38d0bb24bb3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'role': 'assistant',\n",
              " 'content': \"Gross domestic product (GDP) is a monetary measure of the market value of all final goods and services produced in a period (quarterly or yearly) of time. It includes all private and public consumption, government outlays, investments, and exports less imports that occur within a defined territory.\\nWhat is the difference between GDP and GNP?\\nGross domestic product (GDP) is the monetary value of all the finished goods and services produced within a country's borders in a specific time period. Gross national product (GNP) is the monetary value of all the finished goods and services produced by a country's residents in a specific time period.\\nWhat is the difference between GDP and GNP quizlet?\\nGross domestic product (GDP) is the monetary value of all the finished goods and services produced within a country's\"}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2):\n",
        "    df = random.choice(dfs)\n",
        "    ix = len(df)\n",
        "    idx = random.randint(0, ix-1)\n",
        "\n",
        "    question, answer = parse_question_answer(df, idx)\n",
        "\n",
        "    agent_contexts = [[{\"role\": \"user\", \"content\": question}] for agent in range(agents)]"
      ],
      "metadata": {
        "id": "-u0XsrgOkPc0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_contexts[0][0]['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "G2XmSIDNkPey",
        "outputId": "05fcc68f-0744-4acf-f497-4e18259cfede"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Can you answer the following question as accurately as possible? Suppose that 60% of a particular electronic part last over 3 years, while 70% last less than 6 years. Assuming a normal distribution, what are the mean and standard deviation with regard to length of life of these parts?: A) μ = 3.677, σ = 3.561, B) μ = 3.977, σ = 3.861, C) μ = 4.177, σ = 3.561, D) μ = 4.377, σ = 3.261 Explain your answer, putting the answer in the form (X) at the end of your response.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question ="
      ],
      "metadata": {
        "id": "KNDQd6DpoQHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message = construct_message(agent_contexts, question, 2 * round - 1)"
      ],
      "metadata": {
        "id": "FDdsT3hWoFdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fkdYnQFXoFgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l2W5CI6qoFjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZfGHMCKXoFka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P1WHpoVkoFm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your input context\n",
        "input_context = \"could you explain what is GDP?\"\n",
        "\n",
        "# Tokenize the input context\n",
        "input_ids = tokenizer.encode(input_context, return_tensors='pt').to(device)\n",
        "\n",
        "# Generate a response\n",
        "# You can adjust the generation parameters as needed.\n",
        "output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_length=500,\n",
        "            num_beams=5,\n",
        "            temperature=1,\n",
        "            repetition_penalty= 1.5,\n",
        "            #pad_token_id=tokenizer.eos_token_id\n",
        "\n",
        "            # Set the start token for generation to the EOS token\n",
        "            decoder_start_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "# Decode the generated ids to a stringr\n",
        "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "output_text = output_text[len(input_context):].strip()\n",
        "\n",
        "print(output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyZ5-e0J167V",
        "outputId": "7b3799b4-64d2-4969-ecfe-5ccd63b92ba0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gross domestic product (GDP) is a monetary measure of the market value of all final goods and services produced in a period (quarterly or yearly) of time. It includes all private and public consumption, government outlays, investments and exports that occur within a defined territory.[1]\n",
            "GDP per capita is often considered an indicator of a standard of living of a population in a specific period. A country's GDP per capita is calculated by dividing its GDP by its total population.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_text"
      ],
      "metadata": {
        "id": "YK5oWwNvFrUf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "bdc3857f-8001-4ca6-f8e4-1994295989f5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Gross domestic product (GDP) is a monetary measure of the market value of all final goods and services produced in a period (quarterly or yearly) of time. It includes all private and public consumption, government outlays, investments and exports that occur within a defined territory.[1]\\nGDP per capita is often considered an indicator of a standard of living of a population in a specific period. A country's GDP per capita is calculated by dividing its GDP by its total population.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWTYh6JYSDxH",
        "outputId": "77b15324-b34a-469d-fc0b-d2ec945226b2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    1,  1033,   366,  5649,   825,   338,   402, 11191, 29973,    13,\n",
              "         29954,  2124, 21849,  3234,   313, 29954, 11191, 29897,   338,   263,\n",
              "          1601,   300,   653,  5645,   310,   278,  9999,   995,   310,   599,\n",
              "          2186, 22535,   322,  5786,  7371,   297,   263,  3785,   313,   339,\n",
              "          4254,   368,   470,  1629,   368, 29897,   310,   931, 29889,   739,\n",
              "          7805,   599,  2024,   322,   970, 27430, 29892,  5874,   714, 29880,\n",
              "          1036, 29892, 13258,  1860,   322, 29586,   393,  6403,  2629,   263,\n",
              "          3342, 20123,  7226, 29896, 29962,    13, 29954, 11191,   639,  2117,\n",
              "          2028,   338,  4049,  5545,   385, 27717,   310,   263,  3918,   310,\n",
              "          8471,   310,   263,  4665,   297,   263,  2702,  3785, 29889,   319,\n",
              "          4234, 29915, 29879,   402, 11191,   639,  2117,  2028,   338, 12833,\n",
              "           491,  1933,  4821,   967,   402, 11191,   491,   967,  3001,  4665,\n",
              "         29889,     2]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yweU97dGSDqx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}